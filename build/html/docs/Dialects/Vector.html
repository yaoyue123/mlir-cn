<!DOCTYPE html>
<html  lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

      <title>‘vector’ Dialect</title>
    
          <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
          <link rel="stylesheet" href="../../_static/theme.css " type="text/css" />
      
      <!-- sphinx script_files -->
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
        <script src="../../_static/translations.js"></script>

      
      <!-- bundled in js (rollup iife) -->
      <!-- <script src="../../_static/theme-vendors.js"></script> -->
      <script src="../../_static/theme.js" defer></script>
    
  <link rel="index" title="索引" href="../../genindex.html" />
  <link rel="search" title="搜索" href="../../search.html" /> 
  </head>

  <body>
    <div id="app">
    <div class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="../../index.html" class="home-link">
    
      <span class="site-name">MLIR 中文文档</span>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">



    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            



            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">快速搜索</span>
    <div class="searchformwrapper">
      <form class="search" action="../../search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="搜索" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="../../index.html#mlir">欢迎使用 mlir 中文文档</a></span>
      </p>
      <ul class="">
        
          <li class="toctree-l1 ">
            
              <a href="../../_index.html" class="reference internal ">开始使用MLIR</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../Tutorials/CreatingADialect.html" class="reference internal ">Creating a Dialect</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../Tutorials/DataFlowAnalysis.html" class="reference internal ">Writing DataFlow Analyses in MLIR</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../Tutorials/QuickstartRewrites.html" class="reference internal ">Quickstart tutorial to adding MLIR graph rewrite</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../Tutorials/Toy/Ch-1.html" class="reference internal ">第1章：Toy语言和AST（抽象语法树）</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../Tutorials/Toy/Ch-2.html" class="reference internal ">Chapter 2: Emitting Basic MLIR</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../Tutorials/Toy/Ch-3.html" class="reference internal ">Chapter 3: High-level Language-Specific Analysis and Transformation</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../Tutorials/Toy/Ch-4.html" class="reference internal ">Chapter 4: Enabling Generic Transformation with Interfaces</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../Tutorials/Toy/Ch-5.html" class="reference internal ">Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../Tutorials/Toy/Ch-6.html" class="reference internal ">Chapter 6: Lowering to LLVM and CodeGeneration</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../Tutorials/Toy/Ch-7.html" class="reference internal ">Chapter 7: Adding a Composite Type to Toy</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../Tutorials/Toy/_index.html" class="reference internal ">Toy 入门教程</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../Tutorials/UnderstandingTheIRStructure.html" class="reference internal ">Understanding the IR Structure</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../Tutorials/_index.html" class="reference internal ">Tutorials</a>
            

            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="../../index.html">Docs</a> &raquo;</li>
    
    <li>‘vector’ Dialect</li>
  </ul>
  

  <ul class="page-nav">
</ul>
  
</div>
<hr>
          <div class="content" role="main" v-pre>
            
  <section id="vector-dialect">
<h1>‘vector’ Dialect<a class="headerlink" href="#vector-dialect" title="此标题的永久链接">¶</a></h1>
<p>[TOC]</p>
<p>MLIR supports multi-dimensional <code class="docutils literal notranslate"><span class="pre">vector</span></code> types and custom operations on those
types. A generic, retargetable, higher-order <code class="docutils literal notranslate"><span class="pre">vector</span></code> type (<code class="docutils literal notranslate"><span class="pre">n-D</span></code> with <code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>)
is a structured type, that carries semantic information useful for
transformations. This document discusses retargetable abstractions that exist in
MLIR today and operate on ssa-values of type <code class="docutils literal notranslate"><span class="pre">vector</span></code> along with pattern
rewrites and lowerings that enable targeting specific instructions on concrete
targets. These abstractions serve to separate concerns between operations on
<code class="docutils literal notranslate"><span class="pre">memref</span></code> (a.k.a buffers) and operations on <code class="docutils literal notranslate"><span class="pre">vector</span></code> values. This is not a new
proposal but rather a textual documentation of existing MLIR components along
with a rationale.</p>
<section id="positioning-in-the-codegen-infrastructure">
<h2>Positioning in the Codegen Infrastructure<a class="headerlink" href="#positioning-in-the-codegen-infrastructure" title="此标题的永久链接">¶</a></h2>
<p>The following diagram, recently presented with the
<a class="reference external" href="https://drive.google.com/corp/drive/u/0/folders/1sRAsgsd8Bvpm_IxREmZf2agsGU2KvrK-">StructuredOps abstractions</a>,
captures the current codegen paths implemented in MLIR in the various existing
lowering paths.
<img alt="https://user-images.githubusercontent.com/10148468/71177417-f78e4d80-2239-11ea-92ef-700f42ea503f.png" src="https://user-images.githubusercontent.com/10148468/71177417-f78e4d80-2239-11ea-92ef-700f42ea503f.png" /></p>
<p>The following diagram seeks to isolate <code class="docutils literal notranslate"><span class="pre">vector</span></code> dialects from the complexity of
the codegen paths and focus on the payload-carrying ops that operate on std and
<code class="docutils literal notranslate"><span class="pre">vector</span></code> types. This diagram is not to be taken as set in stone and
representative of what exists today but rather illustrates the layering of
abstractions in MLIR.</p>
<p><img alt="vector Abstractions in MLIR" src="https://user-images.githubusercontent.com/10148468/71176949-e85ad000-2238-11ea-9806-200843bc4943.png" /><code class="docutils literal notranslate"></code></p>
<p>This  separates concerns related to (a) defining efficient operations on
<code class="docutils literal notranslate"><span class="pre">vector</span></code> types from (b) program analyses + transformations on <code class="docutils literal notranslate"><span class="pre">memref</span></code>, loops
and other types of structured ops (be they <code class="docutils literal notranslate"><span class="pre">HLO</span></code>, <code class="docutils literal notranslate"><span class="pre">LHLO</span></code>, <code class="docutils literal notranslate"><span class="pre">Linalg</span></code> or other ).
Looking a bit forward in time, we can put a stake in the ground and venture that
the higher level of <code class="docutils literal notranslate"><span class="pre">vector</span></code>-level primitives we build and target from codegen
(or some user/language level), the simpler our task will be, the more complex
patterns can be expressed and the better performance will be.</p>
</section>
<section id="components-of-a-generic-retargetable-vector-level-dialect">
<h2>Components of a Generic Retargetable Vector-Level Dialect<a class="headerlink" href="#components-of-a-generic-retargetable-vector-level-dialect" title="此标题的永久链接">¶</a></h2>
<p>The existing MLIR <code class="docutils literal notranslate"><span class="pre">vector</span></code>-level dialects are related to the following bottom-up
abstractions:</p>
<ol class="simple">
<li><p>Representation in <code class="docutils literal notranslate"><span class="pre">LLVMIR</span></code> via data structures, instructions and intrinsics.
This is referred to as the <code class="docutils literal notranslate"><span class="pre">LLVM</span></code> level.</p></li>
<li><p>Set of machine-specific operations and types that are built to translate
almost 1-1 with the HW ISA. This is referred to as the Hardware Vector
level; a.k.a <code class="docutils literal notranslate"><span class="pre">HWV</span></code>. For instance, we have (a) the <code class="docutils literal notranslate"><span class="pre">NVVM</span></code> dialect (for
<code class="docutils literal notranslate"><span class="pre">CUDA</span></code>) with tensor core ops, (b) accelerator-specific dialects (internal),
a potential (future) <code class="docutils literal notranslate"><span class="pre">CPU</span></code> dialect to capture <code class="docutils literal notranslate"><span class="pre">LLVM</span></code> intrinsics more closely
and other dialects for specific hardware. Ideally this should be
auto-generated as much as possible from the <code class="docutils literal notranslate"><span class="pre">LLVM</span></code> level.</p></li>
<li><p>Set of virtual, machine-agnostic, operations that are informed by costs at
the <code class="docutils literal notranslate"><span class="pre">HWV</span></code>-level. This is referred to as the Virtual Vector level; a.k.a
<code class="docutils literal notranslate"><span class="pre">VV</span></code>. This is the level that higher-level abstractions (codegen, automatic
vectorization, potential vector language, …) targets.</p></li>
</ol>
<p>The existing generic, retargetable, <code class="docutils literal notranslate"><span class="pre">vector</span></code>-level dialect is related to the
following top-down rewrites and conversions:</p>
<ol class="simple">
<li><p>MLIR Rewrite Patterns applied by the MLIR <code class="docutils literal notranslate"><span class="pre">PatternRewrite</span></code> infrastructure to
progressively lower to implementations that match closer and closer to the
<code class="docutils literal notranslate"><span class="pre">HWV</span></code>. Some patterns are “in-dialect” <code class="docutils literal notranslate"><span class="pre">VV</span> <span class="pre">-&gt;</span> <span class="pre">VV</span></code> and some are conversions
<code class="docutils literal notranslate"><span class="pre">VV</span> <span class="pre">-&gt;</span> <span class="pre">HWV</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Virtual</span> <span class="pre">Vector</span> <span class="pre">-&gt;</span> <span class="pre">Hardware</span> <span class="pre">Vector</span></code> lowering is specified as a set of MLIR
lowering patterns that are specified manually for now.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Hardware</span> <span class="pre">Vector</span> <span class="pre">-&gt;</span> <span class="pre">LLVM</span></code> lowering is a mechanical process that is written
manually at the moment and that should be automated, following the <code class="docutils literal notranslate"><span class="pre">LLVM</span> <span class="pre">-&gt;</span> <span class="pre">Hardware</span> <span class="pre">Vector</span></code> ops generation as closely as possible.</p></li>
</ol>
</section>
<section id="short-description-of-the-existing-infrastructure">
<h2>Short Description of the Existing Infrastructure<a class="headerlink" href="#short-description-of-the-existing-infrastructure" title="此标题的永久链接">¶</a></h2>
<section id="llvm-level">
<h3>LLVM level<a class="headerlink" href="#llvm-level" title="此标题的永久链接">¶</a></h3>
<p>On CPU, the <code class="docutils literal notranslate"><span class="pre">n-D</span></code> <code class="docutils literal notranslate"><span class="pre">vector</span></code> type currently lowers to <code class="docutils literal notranslate"><span class="pre">!llvm&lt;array&lt;vector&gt;&gt;</span></code>. More
concretely, <code class="docutils literal notranslate"><span class="pre">vector&lt;4x8x128xf32&gt;</span></code> lowers to <code class="docutils literal notranslate"><span class="pre">!llvm&lt;[4</span> <span class="pre">x</span> <span class="pre">[</span> <span class="pre">8</span> <span class="pre">x</span> <span class="pre">[</span> <span class="pre">128</span> <span class="pre">x</span> <span class="pre">float</span> <span class="pre">]]]&gt;</span></code>. There are tradeoffs involved related to how one can access subvectors and
how one uses <code class="docutils literal notranslate"><span class="pre">llvm.extractelement</span></code>, <code class="docutils literal notranslate"><span class="pre">llvm.insertelement</span></code> and
<code class="docutils literal notranslate"><span class="pre">llvm.shufflevector</span></code>. A <a class="reference external" href="#DeeperDive">deeper dive section</a> discusses the current
lowering choices and tradeoffs.</p>
</section>
<section id="hardware-vector-ops">
<h3>Hardware Vector Ops<a class="headerlink" href="#hardware-vector-ops" title="此标题的永久链接">¶</a></h3>
<p>Hardware Vector Ops are implemented as one dialect per target. For internal
hardware, we are auto-generating the specific HW dialects. For <code class="docutils literal notranslate"><span class="pre">GPU</span></code>, the <code class="docutils literal notranslate"><span class="pre">NVVM</span></code>
dialect adds operations such as <code class="docutils literal notranslate"><span class="pre">mma.sync</span></code>, <code class="docutils literal notranslate"><span class="pre">shfl</span></code> and tests. For <code class="docutils literal notranslate"><span class="pre">CPU</span></code> things
are somewhat in-flight because the abstraction is close to <code class="docutils literal notranslate"><span class="pre">LLVMIR</span></code>. The jury is
still out on  whether a generic <code class="docutils literal notranslate"><span class="pre">CPU</span></code> dialect is concretely needed, but it seems
reasonable to have the same levels of abstraction for all targets and perform
cost-based lowering decisions in MLIR even for <code class="docutils literal notranslate"><span class="pre">LLVM</span></code>. Specialized <code class="docutils literal notranslate"><span class="pre">CPU</span></code>
dialects that would capture specific features not well captured by LLVM peephole
optimizations of on different types that core MLIR supports (e.g. Scalable
Vectors) are welcome future extensions.</p>
</section>
<section id="virtual-vector-ops">
<h3>Virtual Vector Ops<a class="headerlink" href="#virtual-vector-ops" title="此标题的永久链接">¶</a></h3>
<p>Some existing Arith and Vector Dialect on <code class="docutils literal notranslate"><span class="pre">n-D</span></code> <code class="docutils literal notranslate"><span class="pre">vector</span></code> types comprise:</p>
<div class="highlight-mlir notranslate"><div class="highlight"><pre><span></span>// Produces a vector&lt;3x7x8xf32&gt;
%a = arith.addf %0, %1 : vector&lt;3x7x8xf32&gt;
// Produces a vector&lt;3x7x8xf32&gt;
%b = arith.mulf %0, %1 : vector&lt;3x7x8xf32&gt;
// Produces a vector&lt;3x7x8xf32&gt;
%c = vector.splat %1 : vector&lt;3x7x8xf32&gt;

%d = vector.extract %0[1]: vector&lt;3x7x8xf32&gt;     // -&gt; vector&lt;7x8xf32&gt;
%e = vector.extract %0[1, 5]: vector&lt;3x7x8xf32&gt;  // -&gt; vector&lt;8xf32&gt;
%f = vector.outerproduct %0, %1: vector&lt;4xf32&gt;, vector&lt;8xf32&gt;      // -&gt; vector&lt;4x8xf32&gt;
%g = vector.outerproduct %0, %1, %2: vector&lt;4xf32&gt;, vector&lt;8xf32&gt;  // fma when adding %2

// Returns a slice of type vector&lt;2x2x16xf32&gt;
%h = vector.strided_slice %0
    {offsets = [2, 2], sizes = [2, 2], strides = [1, 1]}:
  vector&lt;4x8x16xf32&gt;

%i = vector.transfer_read %A[%0, %1]
    {permutation_map = (d0, d1) -&gt; (d0)}:
  memref&lt;7x?xf32&gt;, vector&lt;4xf32&gt;

vector.transfer_write %f1, %A[%i0, %i1, %i2, %i3]
    {permutation_map = (d0, d1, d2, d3) -&gt; (d3, d1, d0)} :
  vector&lt;5x4x3xf32&gt;, memref&lt;?x?x?x?xf32&gt;
</pre></div>
</div>
<p>The list of Vector is currently undergoing evolutions and is best kept track of
by following the evolution of the
<a class="reference external" href="https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Vector/IR/VectorOps.td">VectorOps.td</a>
ODS file (markdown documentation is automatically generated locally when
building and populates the
<a class="reference external" href="https://github.com/llvm/llvm-project/blob/main/mlir/docs/Dialects/Vector.md">Vector doc</a>).
Recent extensions are driven by concrete use cases of interest. A notable such
use case is the <code class="docutils literal notranslate"><span class="pre">vector.contract</span></code> op which applies principles of the
StructuredOps abstraction to <code class="docutils literal notranslate"><span class="pre">vector</span></code> types.</p>
</section>
<section id="virtual-vector-rewrite-patterns">
<h3>Virtual Vector Rewrite Patterns<a class="headerlink" href="#virtual-vector-rewrite-patterns" title="此标题的永久链接">¶</a></h3>
<p>The following rewrite patterns exist at the <code class="docutils literal notranslate"><span class="pre">VV-&gt;VV</span></code> level:</p>
<ol class="simple">
<li><p>The now retired <code class="docutils literal notranslate"><span class="pre">MaterializeVector</span></code> pass used to legalize ops on a
coarse-grained virtual <code class="docutils literal notranslate"><span class="pre">vector</span></code> to a finer-grained virtual <code class="docutils literal notranslate"><span class="pre">vector</span></code> by
unrolling. This has been rewritten as a retargetable unroll-and-jam pattern
on <code class="docutils literal notranslate"><span class="pre">vector</span></code> ops and <code class="docutils literal notranslate"><span class="pre">vector</span></code> types.</p></li>
<li><p>The lowering of <code class="docutils literal notranslate"><span class="pre">vector_transfer</span></code> ops legalizes <code class="docutils literal notranslate"><span class="pre">vector</span></code> load/store ops to
permuted loops over scalar load/stores. This should evolve to loops over
<code class="docutils literal notranslate"><span class="pre">vector</span></code> load/stores + <code class="docutils literal notranslate"><span class="pre">mask</span></code> operations as they become available <code class="docutils literal notranslate"><span class="pre">vector</span></code>
ops at the <code class="docutils literal notranslate"><span class="pre">VV</span></code> level.</p></li>
</ol>
<p>The general direction is to add more Virtual Vector level ops and implement more
useful <code class="docutils literal notranslate"><span class="pre">VV</span> <span class="pre">-&gt;</span> <span class="pre">VV</span></code> rewrites as composable patterns that the PatternRewrite
infrastructure can apply iteratively.</p>
</section>
<section id="virtual-vector-to-hardware-vector-lowering">
<h3>Virtual Vector to Hardware Vector Lowering<a class="headerlink" href="#virtual-vector-to-hardware-vector-lowering" title="此标题的永久链接">¶</a></h3>
<p>For now, <code class="docutils literal notranslate"><span class="pre">VV</span> <span class="pre">-&gt;</span> <span class="pre">HWV</span></code> are specified in C++ (see for instance the
<a class="reference external" href="https://github.com/tensorflow/mlir/commit/0a0c4867c6a6fcb0a2f17ef26a791c1d551fe33d">SplatOpLowering for n-D vectors</a>
or the
<a class="reference external" href="https://github.com/tensorflow/mlir/commit/957b1ca9680b4aacabb3a480fbc4ebd2506334b8">VectorOuterProductOp lowering</a>).</p>
<p>Simple
<a class="reference external" href="https://github.com/llvm/llvm-project/blob/main/mlir/test/Conversion/VectorToLLVM/vector-to-llvm.mlir">conversion tests</a>
are available for the <code class="docutils literal notranslate"><span class="pre">LLVM</span></code> target starting from the Virtual Vector Level.</p>
</section>
</section>
<section id="rationale">
<h2>Rationale<a class="headerlink" href="#rationale" title="此标题的永久链接">¶</a></h2>
<section id="hardware-as-vector-machines-of-minimum-granularity">
<h3>Hardware as <code class="docutils literal notranslate"><span class="pre">vector</span></code> Machines of Minimum Granularity<a class="headerlink" href="#hardware-as-vector-machines-of-minimum-granularity" title="此标题的永久链接">¶</a></h3>
<p>Higher-dimensional <code class="docutils literal notranslate"><span class="pre">vector</span></code>s are ubiquitous in modern HPC hardware. One way to
think about Generic Retargetable <code class="docutils literal notranslate"><span class="pre">vector</span></code>-Level Dialect is that it operates on
<code class="docutils literal notranslate"><span class="pre">vector</span></code> types that are multiples of a “good” <code class="docutils literal notranslate"><span class="pre">vector</span></code> size so the HW can
efficiently implement a set of high-level primitives (e.g.
<code class="docutils literal notranslate"><span class="pre">vector&lt;8x8x8x16xf32&gt;</span></code> when HW <code class="docutils literal notranslate"><span class="pre">vector</span></code> size is say <code class="docutils literal notranslate"><span class="pre">vector&lt;4x8xf32&gt;</span></code>).</p>
<p>Some notable <code class="docutils literal notranslate"><span class="pre">vector</span></code> sizes of interest include:</p>
<ol class="simple">
<li><p>CPU: <code class="docutils literal notranslate"><span class="pre">vector&lt;HW_vector_size</span> <span class="pre">*</span> <span class="pre">k&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">vector&lt;core_count</span> <span class="pre">*</span> <span class="pre">k’</span> <span class="pre">x</span> <span class="pre">HW_vector_size</span> <span class="pre">*</span> <span class="pre">k&gt;</span></code> and <code class="docutils literal notranslate"><span class="pre">vector&lt;socket_count</span> <span class="pre">x</span> <span class="pre">core_count</span> <span class="pre">*</span> <span class="pre">k’</span> <span class="pre">x</span> <span class="pre">HW_vector_size</span> <span class="pre">*</span> <span class="pre">k&gt;</span></code></p></li>
<li><p>GPU: <code class="docutils literal notranslate"><span class="pre">vector&lt;warp_size</span> <span class="pre">*</span> <span class="pre">k&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">vector&lt;warp_size</span> <span class="pre">*</span> <span class="pre">k</span> <span class="pre">x</span> <span class="pre">float4&gt;</span></code> and
<code class="docutils literal notranslate"><span class="pre">vector&lt;warp_size</span> <span class="pre">*</span> <span class="pre">k</span> <span class="pre">x</span> <span class="pre">4</span> <span class="pre">x</span> <span class="pre">4</span> <span class="pre">x</span> <span class="pre">4&gt;</span></code> for tensor_core sizes,</p></li>
<li><p>Other accelerators: n-D <code class="docutils literal notranslate"><span class="pre">vector</span></code> as first-class citizens in the HW.</p></li>
</ol>
<p>Depending on the target, ops on sizes that are not multiples of the HW <code class="docutils literal notranslate"><span class="pre">vector</span></code>
size may either produce slow code (e.g. by going through <code class="docutils literal notranslate"><span class="pre">LLVM</span></code> legalization) or
may not legalize at all (e.g. some unsupported accelerator X combination of ops
and types).</p>
</section>
<section id="transformations-problems-avoided">
<h3>Transformations Problems Avoided<a class="headerlink" href="#transformations-problems-avoided" title="此标题的永久链接">¶</a></h3>
<p>A <code class="docutils literal notranslate"><span class="pre">vector&lt;16x32x64xf32&gt;</span></code> virtual <code class="docutils literal notranslate"><span class="pre">vector</span></code> is a coarse-grained type that can be
“unrolled” to HW-specific sizes. The multi-dimensional unrolling factors are
carried in the IR by the <code class="docutils literal notranslate"><span class="pre">vector</span></code> type. After unrolling, traditional
instruction-level scheduling can be run.</p>
<p>The following key transformations (along with the supporting analyses and
structural constraints) are completely avoided by operating on a <code class="docutils literal notranslate"><span class="pre">vector</span></code>
<code class="docutils literal notranslate"><span class="pre">ssa-value</span></code> abstraction:</p>
<ol class="simple">
<li><p>Loop unroll and unroll-and-jam.</p></li>
<li><p>Loop and load-store restructuring for register reuse.</p></li>
<li><p>Load to store forwarding and Mem2reg.</p></li>
<li><p>Coarsening (raising) from finer-grained <code class="docutils literal notranslate"><span class="pre">vector</span></code> form.</p></li>
</ol>
<p>Note that “unrolling” in the context of <code class="docutils literal notranslate"><span class="pre">vector</span></code>s corresponds to partial loop
unroll-and-jam and not full unrolling. As a consequence this is expected to
compose with SW pipelining where applicable and does not result in ICache blow
up.</p>
</section>
<section id="the-big-out-of-scope-piece-automatic-vectorization">
<h3>The Big Out-Of-Scope Piece: Automatic Vectorization<a class="headerlink" href="#the-big-out-of-scope-piece-automatic-vectorization" title="此标题的永久链接">¶</a></h3>
<p>One important piece not discussed here is automatic vectorization (automatically
raising from scalar to n-D <code class="docutils literal notranslate"><span class="pre">vector</span></code> ops and types). The TL;DR is that when the
first “super-vectorization” prototype was implemented, MLIR was nowhere near as
mature as it is today. As we continue building more abstractions in <code class="docutils literal notranslate"><span class="pre">VV</span> <span class="pre">-&gt;</span> <span class="pre">HWV</span></code>,
there is an opportunity to revisit vectorization in MLIR.</p>
<p>Since this topic touches on codegen abstractions, it is technically out of the
scope of this survey document but there is a lot to discuss in light of
structured op type representations and how a vectorization transformation can be
reused across dialects. In particular, MLIR allows the definition of dialects at
arbitrary levels of granularity and lends itself favorably to progressive
lowering. The argument can be made that automatic vectorization on a loops + ops
abstraction is akin to raising structural information that has been lost.
Instead, it is possible to revisit vectorization as simple pattern rewrites,
provided the IR is in a suitable form. For instance, vectorizing a
<code class="docutils literal notranslate"><span class="pre">linalg.generic</span></code> op whose semantics match a <code class="docutils literal notranslate"><span class="pre">matmul</span></code> can be done
<a class="reference external" href="https://github.com/tensorflow/mlir/commit/bff722d6b59ab99b998f0c2b9fccd0267d9f93b5">quite easily with a pattern</a>.
In fact this pattern is trivial to generalize to any type of contraction when
targeting the <code class="docutils literal notranslate"><span class="pre">vector.contract</span></code> op, as well as to any field (<code class="docutils literal notranslate"><span class="pre">+/*</span></code>, <code class="docutils literal notranslate"><span class="pre">min/+</span></code>,
<code class="docutils literal notranslate"><span class="pre">max/+</span></code>, <code class="docutils literal notranslate"><span class="pre">or/and</span></code>, <code class="docutils literal notranslate"><span class="pre">logsumexp/+</span></code> …) . In other words, by operating on a higher
level of generic abstractions than affine loops, non-trivial transformations
become significantly simpler and composable at a finer granularity.</p>
<p>Irrespective of the existence of an auto-vectorizer, one can build a notional
vector language based on the VectorOps dialect and build end-to-end models with
expressing <code class="docutils literal notranslate"><span class="pre">vector</span></code>s in the IR directly and simple pattern-rewrites.
<a class="reference external" href="https://github.com/llvm/llvm-project/blob/main/mlir/docs/EDSC.md">EDSC</a>s
provide a simple way of driving such a notional language directly in C++.</p>
</section>
</section>
<section id="bikeshed-naming-discussion">
<h2>Bikeshed Naming Discussion<a class="headerlink" href="#bikeshed-naming-discussion" title="此标题的永久链接">¶</a></h2>
<p>There are arguments against naming an n-D level of abstraction <code class="docutils literal notranslate"><span class="pre">vector</span></code> because
most people associate it with 1-D <code class="docutils literal notranslate"><span class="pre">vector</span></code>s. On the other hand, <code class="docutils literal notranslate"><span class="pre">vector</span></code>s are
first-class n-D values in MLIR. The alternative name Tile has been proposed,
which conveys higher-D meaning. But it also is one of the most overloaded terms
in compilers and hardware. For now, we generally use the <code class="docutils literal notranslate"><span class="pre">n-D</span></code> <code class="docutils literal notranslate"><span class="pre">vector</span></code> name and
are open to better suggestions.</p>
</section>
<section id="deeperdive">
<h2>DeeperDive<a class="headerlink" href="#deeperdive" title="此标题的永久链接">¶</a></h2>
<p>This section describes the tradeoffs involved in lowering the MLIR n-D vector
type and operations on it to LLVM-IR. Putting aside the
<a class="reference external" href="http://lists.llvm.org/pipermail/llvm-dev/2018-October/126871.html">LLVM Matrix</a>
proposal for now, this assumes LLVM only has built-in support for 1-D vector.
The relationship with the LLVM Matrix proposal is discussed at the end of this
document.</p>
<p>MLIR does not currently support dynamic vector sizes (i.e. SVE style) so the
discussion is limited to static rank and static vector sizes (e.g.
<code class="docutils literal notranslate"><span class="pre">vector&lt;4x8x16x32xf32&gt;</span></code>). This section discusses operations on vectors in LLVM
and MLIR.</p>
<p>LLVM instructions are prefixed by the <code class="docutils literal notranslate"><span class="pre">llvm.</span></code> dialect prefix (e.g.
<code class="docutils literal notranslate"><span class="pre">llvm.insertvalue</span></code>). Such ops operate exclusively on 1-D vectors and aggregates
following the <a class="reference external" href="https://llvm.org/docs/LangRef.html">LLVM LangRef</a>. MLIR
operations are prefixed by the <code class="docutils literal notranslate"><span class="pre">vector.</span></code> dialect prefix (e.g.
<code class="docutils literal notranslate"><span class="pre">vector.insertelement</span></code>). Such ops operate exclusively on MLIR <code class="docutils literal notranslate"><span class="pre">n-D</span></code> <code class="docutils literal notranslate"><span class="pre">vector</span></code>
types.</p>
<section id="alternatives-for-lowering-an-n-d-vector-type-to-llvm">
<h3>Alternatives For Lowering an n-D Vector Type to LLVM<a class="headerlink" href="#alternatives-for-lowering-an-n-d-vector-type-to-llvm" title="此标题的永久链接">¶</a></h3>
<p>Consider a vector of rank n with static sizes <code class="docutils literal notranslate"><span class="pre">{s_0,</span> <span class="pre">...</span> <span class="pre">s_{n-1}}</span></code> (i.e. an MLIR
<code class="docutils literal notranslate"><span class="pre">vector&lt;s_0x...s_{n-1}xf32&gt;</span></code>). Lowering such an <code class="docutils literal notranslate"><span class="pre">n-D</span></code> MLIR vector type to an
LLVM descriptor can be done by either:</p>
<ol class="simple">
<li><p>Flattening to a <code class="docutils literal notranslate"><span class="pre">1-D</span></code> vector: <code class="docutils literal notranslate"><span class="pre">!llvm&lt;&quot;(s_0*...*s_{n-1})xfloat&quot;&gt;</span></code> in the MLIR
LLVM dialect.</p></li>
<li><p>Nested aggregate type of <code class="docutils literal notranslate"><span class="pre">1-D</span></code> vector:
<code class="docutils literal notranslate"><span class="pre">!llvm.&quot;[s_0x[s_1x[...&lt;s_{n-1}xf32&gt;]]]&quot;&gt;</span></code> in the MLIR LLVM dialect.</p></li>
<li><p>A mix of both.</p></li>
</ol>
<p>There are multiple tradeoffs involved in choosing one or the other that we
discuss. It is important to note that “a mix of both” immediately reduces to
“nested aggregate type of 1-D vector” with a <code class="docutils literal notranslate"><span class="pre">vector.cast</span> <span class="pre">%0:</span> <span class="pre">vector&lt;4x8x16x32xf32&gt;</span> <span class="pre">to</span> <span class="pre">vector&lt;4x4096xf32&gt;</span></code> operation, that flattens the most
“k” minor dimensions.</p>
</section>
<section id="constraints-inherited-from-llvm-see-langref">
<h3>Constraints Inherited from LLVM (see LangRef)<a class="headerlink" href="#constraints-inherited-from-llvm-see-langref" title="此标题的永久链接">¶</a></h3>
<p>The first constraint was already mentioned: LLVM only supports <code class="docutils literal notranslate"><span class="pre">1-D</span></code> <code class="docutils literal notranslate"><span class="pre">vector</span></code>
types natively. Additional constraints are related to the difference in LLVM
between vector and aggregate types: <code class="docutils literal notranslate"><span class="pre">“Aggregate</span> <span class="pre">Types</span> <span class="pre">are</span> <span class="pre">a</span> <span class="pre">subset</span> <span class="pre">of</span> <span class="pre">derived</span> <span class="pre">types</span> <span class="pre">that</span> <span class="pre">can</span> <span class="pre">contain</span> <span class="pre">multiple</span> <span class="pre">member</span> <span class="pre">types.</span> <span class="pre">Arrays</span> <span class="pre">and</span> <span class="pre">structs</span> <span class="pre">are</span> <span class="pre">aggregate</span> <span class="pre">types.</span> <span class="pre">Vectors</span> <span class="pre">are</span> <span class="pre">not</span> <span class="pre">considered</span> <span class="pre">to</span> <span class="pre">be</span> <span class="pre">aggregate</span> <span class="pre">types.”.</span></code></p>
<p>This distinction is also reflected in some of the operations. For <code class="docutils literal notranslate"><span class="pre">1-D</span></code> vectors,
the operations <code class="docutils literal notranslate"><span class="pre">llvm.extractelement</span></code>, <code class="docutils literal notranslate"><span class="pre">llvm.insertelement</span></code>, and
<code class="docutils literal notranslate"><span class="pre">llvm.shufflevector</span></code> apply, with direct support for dynamic indices. For <code class="docutils literal notranslate"><span class="pre">n-D</span></code>
vectors with <code class="docutils literal notranslate"><span class="pre">n&gt;1</span></code>, and thus aggregate types at LLVM level, the more restrictive
operations <code class="docutils literal notranslate"><span class="pre">llvm.extractvalue</span></code> and <code class="docutils literal notranslate"><span class="pre">llvm.insertvalue</span></code> apply, which only accept
static indices. There is no direct shuffling support for aggregate types.</p>
<p>The next sentence illustrates a recurrent tradeoff, also found in MLIR, between
“value types” (subject to SSA use-def chains) and “memory types” (subject to
aliasing and side-effects): <code class="docutils literal notranslate"><span class="pre">“Structures</span> <span class="pre">in</span> <span class="pre">memory</span> <span class="pre">are</span> <span class="pre">accessed</span> <span class="pre">using</span> <span class="pre">‘load’</span> <span class="pre">and</span> <span class="pre">‘store’</span> <span class="pre">by</span> <span class="pre">getting</span> <span class="pre">a</span> <span class="pre">pointer</span> <span class="pre">to</span> <span class="pre">a</span> <span class="pre">field</span> <span class="pre">with</span> <span class="pre">the</span> <span class="pre">llvm.getelementptr</span> <span class="pre">instruction.</span> <span class="pre">Structures</span> <span class="pre">in</span> <span class="pre">registers</span> <span class="pre">are</span> <span class="pre">accessed</span> <span class="pre">using</span> <span class="pre">the</span> <span class="pre">llvm.extractvalue</span> <span class="pre">and</span> <span class="pre">llvm.insertvalue</span> <span class="pre">instructions.”</span></code></p>
<p>When transposing this to MLIR, <code class="docutils literal notranslate"><span class="pre">llvm.getelementptr</span></code> works on pointers to <code class="docutils literal notranslate"><span class="pre">n-D</span></code>
vectors in memory. For <code class="docutils literal notranslate"><span class="pre">n-D</span></code>, vectors values that live in registers we can use
<code class="docutils literal notranslate"><span class="pre">vector.extract</span></code> and <code class="docutils literal notranslate"><span class="pre">vector.insert</span></code> which do not accept dynamic indices. Note
that this is consistent with hardware considerations as discussed below.</p>
<p>An alternative is to use an LLVM <code class="docutils literal notranslate"><span class="pre">1-D</span></code> <code class="docutils literal notranslate"><span class="pre">vector</span></code> type for which one can use
<code class="docutils literal notranslate"><span class="pre">llvm.extractelement</span></code>, <code class="docutils literal notranslate"><span class="pre">llvm.insertelement</span></code> and <code class="docutils literal notranslate"><span class="pre">llvm.shufflevector</span></code>. These
operations accept dynamic indices. The implication is that one has to use a
flattened lowering of an MLIR n-D vector to an LLVM 1-D vector.</p>
<p>There are multiple tradeoffs involved that mix implications on the programming
model, execution on actual HW and what is visible or hidden from codegen. They
are discussed in the following sections.</p>
</section>
<section id="nested-aggregate">
<h3>Nested Aggregate<a class="headerlink" href="#nested-aggregate" title="此标题的永久链接">¶</a></h3>
<p>Pros:</p>
<ol class="simple">
<li><p>Natural encoding n-D vector -&gt; (n-1)-D aggregate over 1-D vector.</p></li>
<li><p>No need for linearization / delinearization logic inserted everywhere.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">llvm.insertvalue</span></code>, <code class="docutils literal notranslate"><span class="pre">llvm.extractvalue</span></code> of <code class="docutils literal notranslate"><span class="pre">(n-k)-D</span></code> aggregate is natural.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">llvm.insertelement</span></code>, <code class="docutils literal notranslate"><span class="pre">llvm.extractelement</span></code>, <code class="docutils literal notranslate"><span class="pre">llvm.shufflevector</span></code> over <code class="docutils literal notranslate"><span class="pre">1-D</span></code>
vector type is natural.</p></li>
</ol>
<p>Cons:</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">llvm.insertvalue</span></code> / <code class="docutils literal notranslate"><span class="pre">llvm.extractvalue</span></code> does not accept dynamic indices but
only static ones.</p></li>
<li><p>Dynamic indexing on the non-most-minor dimension requires roundtrips to
memory.</p></li>
<li><p>Special intrinsics and native instructions in LLVM operate on <code class="docutils literal notranslate"><span class="pre">1-D</span></code> vectors.
This is not expected to be a practical limitation thanks to a <code class="docutils literal notranslate"><span class="pre">vector.cast</span> <span class="pre">%0:</span> <span class="pre">vector&lt;4x8x16x32xf32&gt;</span> <span class="pre">to</span> <span class="pre">vector&lt;4x4096xf32&gt;</span></code> operation, that flattens
the most minor dimensions (see the bigger picture in implications on
codegen).</p></li>
</ol>
</section>
<section id="flattened-1-d-vector-type">
<h3>Flattened 1-D Vector Type<a class="headerlink" href="#flattened-1-d-vector-type" title="此标题的永久链接">¶</a></h3>
<p>Pros:</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">insertelement</span></code> / <code class="docutils literal notranslate"><span class="pre">extractelement</span></code> / <code class="docutils literal notranslate"><span class="pre">shufflevector</span></code> with dynamic indexing
is possible over the whole lowered <code class="docutils literal notranslate"><span class="pre">n-D</span></code> vector type.</p></li>
<li><p>Supports special intrinsics and native operations.</p></li>
</ol>
<p>Cons:</p>
<ol class="simple">
<li><p>Requires linearization/delinearization logic everywhere, translations are
complex.</p></li>
<li><p>Hides away the real HW structure behind dynamic indexing: at the end of the
day, HW vector sizes are generally fixed and multiple vectors will be needed
to hold a vector that is larger than the HW.</p></li>
<li><p>Unlikely peephole optimizations will result in good code: arbitrary dynamic
accesses, especially at HW vector boundaries unlikely to result in regular
patterns.</p></li>
</ol>
</section>
<section id="discussion">
<h3>Discussion<a class="headerlink" href="#discussion" title="此标题的永久链接">¶</a></h3>
<section id="hw-vectors-and-implications-on-the-sw-and-the-programming-model">
<h4>HW Vectors and Implications on the SW and the Programming Model<a class="headerlink" href="#hw-vectors-and-implications-on-the-sw-and-the-programming-model" title="此标题的永久链接">¶</a></h4>
<p>As of today, the LLVM model only support <code class="docutils literal notranslate"><span class="pre">1-D</span></code> vector types. This is
unsurprising because historically, the vast majority of HW only supports <code class="docutils literal notranslate"><span class="pre">1-D</span></code>
vector registers. We note that multiple HW vendors are in the process of
evolving to higher-dimensional physical vectors.</p>
<p>In the following discussion, let’s assume the HW vector size is <code class="docutils literal notranslate"><span class="pre">1-D</span></code> and the SW
vector size is <code class="docutils literal notranslate"><span class="pre">n-D</span></code>, with <code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">&gt;=</span> <span class="pre">1</span></code>. The same discussion would apply with <code class="docutils literal notranslate"><span class="pre">2-D</span></code>
HW <code class="docutils literal notranslate"><span class="pre">vector</span></code> size and <code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">&gt;=</span> <span class="pre">2</span></code>. In this context, most HW exhibit a vector
register file. The number of such vectors is fixed. Depending on the rank and
sizes of the SW vector abstraction and the HW vector sizes and number of
registers, an <code class="docutils literal notranslate"><span class="pre">n-D</span></code> SW vector type may be materialized by a mix of multiple
<code class="docutils literal notranslate"><span class="pre">1-D</span></code> HW vector registers + memory locations at a given point in time.</p>
<p>The implication of the physical HW constraints on the programming model are that
one cannot index dynamically across hardware registers: a register file can
generally not be indexed dynamically. This is because the register number is
fixed and one either needs to unroll explicitly to obtain fixed register numbers
or go through memory. This is a constraint familiar to CUDA programmers: when
declaring a <code class="docutils literal notranslate"><span class="pre">private</span> <span class="pre">float</span> <span class="pre">a[4]</span></code>; and subsequently indexing with a <em>dynamic</em>
value results in so-called <strong>local memory</strong> usage (i.e. roundtripping to
memory).</p>
</section>
<section id="implication-on-codegen">
<h4>Implication on codegen<a class="headerlink" href="#implication-on-codegen" title="此标题的永久链接">¶</a></h4>
<p>MLIR <code class="docutils literal notranslate"><span class="pre">n-D</span></code> vector types are currently represented as <code class="docutils literal notranslate"><span class="pre">(n-1)-D</span></code> arrays of <code class="docutils literal notranslate"><span class="pre">1-D</span></code>
vectors when lowered to LLVM. This introduces the consequences on static vs
dynamic indexing discussed previously: <code class="docutils literal notranslate"><span class="pre">extractelement</span></code>, <code class="docutils literal notranslate"><span class="pre">insertelement</span></code> and
<code class="docutils literal notranslate"><span class="pre">shufflevector</span></code> on <code class="docutils literal notranslate"><span class="pre">n-D</span></code> vectors in MLIR only support static indices. Dynamic
indices are only supported on the most minor <code class="docutils literal notranslate"><span class="pre">1-D</span></code> vector but not the outer
<code class="docutils literal notranslate"><span class="pre">(n-1)-D</span></code>. For other cases, explicit load / stores are required.</p>
<p>The implications on codegen are as follows:</p>
<ol class="simple">
<li><p>Loops around <code class="docutils literal notranslate"><span class="pre">vector</span></code> values are indirect addressing of vector values, they
must operate on explicit load / store operations over <code class="docutils literal notranslate"><span class="pre">n-D</span></code> vector types.</p></li>
<li><p>Once an <code class="docutils literal notranslate"><span class="pre">n-D</span></code> <code class="docutils literal notranslate"><span class="pre">vector</span></code> type is loaded into an SSA value (that may or may not
live in <code class="docutils literal notranslate"><span class="pre">n</span></code> registers, with or without spilling, when eventually lowered),
it may be unrolled to smaller <code class="docutils literal notranslate"><span class="pre">k-D</span></code> <code class="docutils literal notranslate"><span class="pre">vector</span></code> types and operations that
correspond to the HW. This level of MLIR codegen is related to register
allocation and spilling that occur much later in the LLVM pipeline.</p></li>
<li><p>HW may support &gt;1-D vectors with intrinsics for indirect addressing within
these vectors. These can be targeted thanks to explicit <code class="docutils literal notranslate"><span class="pre">vector_cast</span></code>
operations from MLIR <code class="docutils literal notranslate"><span class="pre">k-D</span></code> vector types and operations to LLVM <code class="docutils literal notranslate"><span class="pre">1-D</span></code>
vectors + intrinsics.</p></li>
</ol>
<p>Alternatively, we argue that directly lowering to a linearized abstraction hides
away the codegen complexities related to memory accesses by giving a false
impression of magical dynamic indexing across registers. Instead we prefer to
make those very explicit in MLIR and allow codegen to explore tradeoffs.
Different HW will require different tradeoffs in the sizes involved in steps 1.,
2. and 3.</p>
<p>Decisions made at the MLIR level will have implications at a much later stage in
LLVM (after register allocation). We do not envision to expose concerns related
to modeling of register allocation and spilling to MLIR explicitly. Instead,
each target will expose a set of “good” target operations and <code class="docutils literal notranslate"><span class="pre">n-D</span></code> vector
types, associated with costs that <code class="docutils literal notranslate"><span class="pre">PatterRewriters</span></code> at the MLIR level will be
able to target. Such costs at the MLIR level will be abstract and used for
ranking, not for accurate performance modeling. In the future such costs will be
learned.</p>
</section>
<section id="implication-on-lowering-to-accelerators">
<h4>Implication on Lowering to Accelerators<a class="headerlink" href="#implication-on-lowering-to-accelerators" title="此标题的永久链接">¶</a></h4>
<p>To target accelerators that support higher dimensional vectors natively, we can
start from either <code class="docutils literal notranslate"><span class="pre">1-D</span></code> or <code class="docutils literal notranslate"><span class="pre">n-D</span></code> vectors in MLIR and use <code class="docutils literal notranslate"><span class="pre">vector.cast</span></code> to
flatten the most minor dimensions to <code class="docutils literal notranslate"><span class="pre">1-D</span></code> <code class="docutils literal notranslate"><span class="pre">vector&lt;Kxf32&gt;</span></code> where <code class="docutils literal notranslate"><span class="pre">K</span></code> is an
appropriate constant. Then, the existing lowering to LLVM-IR immediately
applies, with extensions for accelerator-specific intrinsics.</p>
<p>It is the role of an Accelerator-specific vector dialect (see codegen flow in
the figure above) to lower the <code class="docutils literal notranslate"><span class="pre">vector.cast</span></code>. Accelerator -&gt; LLVM lowering would
then consist of a bunch of <code class="docutils literal notranslate"><span class="pre">Accelerator</span> <span class="pre">-&gt;</span> <span class="pre">Accelerator</span></code> rewrites to perform the
casts composed with <code class="docutils literal notranslate"><span class="pre">Accelerator</span> <span class="pre">-&gt;</span> <span class="pre">LLVM</span></code> conversions + intrinsics that operate
on <code class="docutils literal notranslate"><span class="pre">1-D</span></code> <code class="docutils literal notranslate"><span class="pre">vector&lt;Kxf32&gt;</span></code>.</p>
<p>Some of those rewrites may need extra handling, especially if a reduction is
involved. For example, <code class="docutils literal notranslate"><span class="pre">vector.cast</span> <span class="pre">%0:</span> <span class="pre">vector&lt;K1x...xKnxf32&gt;</span> <span class="pre">to</span> <span class="pre">vector&lt;Kxf32&gt;</span></code>
when <code class="docutils literal notranslate"><span class="pre">K</span> <span class="pre">!=</span> <span class="pre">K1</span> <span class="pre">*</span> <span class="pre">…</span> <span class="pre">*</span> <span class="pre">Kn</span></code> and some arbitrary irregular <code class="docutils literal notranslate"><span class="pre">vector.cast</span> <span class="pre">%0:</span> <span class="pre">vector&lt;4x4x17xf32&gt;</span> <span class="pre">to</span> <span class="pre">vector&lt;Kxf32&gt;</span></code> may introduce masking and intra-vector
shuffling that may not be worthwhile or even feasible, i.e. infinite cost.</p>
<p>However <code class="docutils literal notranslate"><span class="pre">vector.cast</span> <span class="pre">%0:</span> <span class="pre">vector&lt;K1x...xKnxf32&gt;</span> <span class="pre">to</span> <span class="pre">vector&lt;Kxf32&gt;</span></code> when <code class="docutils literal notranslate"><span class="pre">K</span> <span class="pre">=</span> <span class="pre">K1</span> <span class="pre">*</span> <span class="pre">…</span> <span class="pre">*</span> <span class="pre">Kn</span></code> should be close to a noop.</p>
<p>As we start building accelerator-specific abstractions, we hope to achieve
retargetable codegen: the same infra is used for CPU, GPU and accelerators with
extra MLIR patterns and costs.</p>
</section>
<section id="implication-on-calling-external-functions-that-operate-on-vectors">
<h4>Implication on calling external functions that operate on vectors<a class="headerlink" href="#implication-on-calling-external-functions-that-operate-on-vectors" title="此标题的永久链接">¶</a></h4>
<p>It is possible (likely) that we additionally need to linearize when calling an
external function.</p>
</section>
</section>
<section id="relationship-to-llvm-matrix-type-proposal">
<h3>Relationship to LLVM matrix type proposal.<a class="headerlink" href="#relationship-to-llvm-matrix-type-proposal" title="此标题的永久链接">¶</a></h3>
<p>The LLVM matrix proposal was formulated 1 year ago but seemed to be somewhat
stalled until recently. In its current form, it is limited to 2-D matrix types
and operations are implemented with LLVM intrinsics. In contrast, MLIR sits at a
higher level of abstraction and allows the lowering of generic operations on
generic n-D vector types from MLIR to aggregates of 1-D LLVM vectors. In the
future, it could make sense to lower to the LLVM matrix abstraction also for CPU
even though MLIR will continue needing higher level abstractions.</p>
<p>On the other hand, one should note that as MLIR is moving to LLVM, this document
could become the unifying abstraction that people should target for 1-D vectors
and the LLVM matrix proposal can be viewed as a subset of this work.</p>
</section>
<section id="conclusion">
<h3>Conclusion<a class="headerlink" href="#conclusion" title="此标题的永久链接">¶</a></h3>
<p>The flattened 1-D vector design in the LLVM matrix proposal is good in a
HW-specific world with special intrinsics. This is a good abstraction for
register allocation, Instruction-Level-Parallelism and
SoftWare-Pipelining/Modulo Scheduling optimizations at the register level.
However MLIR codegen operates at a higher level of abstraction where we want to
target operations on coarser-grained vectors than the HW size and on which
unroll-and-jam is applied and patterns across multiple HW vectors can be
matched.</p>
<p>This makes “nested aggregate type of 1-D vector” an appealing abstraction for
lowering from MLIR because:</p>
<ol class="simple">
<li><p>it does not hide complexity related to the buffer vs value semantics and the
memory subsystem and</p></li>
<li><p>it does not rely on LLVM to magically make all the things work from a too
low-level abstraction.</p></li>
</ol>
<p>The use of special intrinsics in a <code class="docutils literal notranslate"><span class="pre">1-D</span></code> LLVM world is still available thanks to
an explicit <code class="docutils literal notranslate"><span class="pre">vector.cast</span></code> op.</p>
</section>
</section>
<section id="operations">
<h2>Operations<a class="headerlink" href="#operations" title="此标题的永久链接">¶</a></h2>
<p>[include “Dialects/VectorOps.md”]</p>
</section>
</section>


          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
</ul><div class="footer" role="contentinfo">
      &#169; 版权所有 2023, yaoyue123.
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 6.1.3 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a> 0.8.0.
</div>
            </div>
          </div>
      </page>
    </div></div>
    
    
  </body>
</html>