<!DOCTYPE html>
<html  lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

      <title>Chapter 4: Enabling Generic Transformation with Interfaces</title>
    
          <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
          <link rel="stylesheet" href="../../../_static/theme.css " type="text/css" />
      
      <!-- sphinx script_files -->
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
        <script src="../../../_static/translations.js"></script>

      
      <!-- bundled in js (rollup iife) -->
      <!-- <script src="../../../_static/theme-vendors.js"></script> -->
      <script src="../../../_static/theme.js" defer></script>
    
  <link rel="index" title="索引" href="../../../genindex.html" />
  <link rel="search" title="搜索" href="../../../search.html" />
  <link rel="next" title="Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization" href="Ch-5.html" />
  <link rel="prev" title="Chapter 3: High-level Language-Specific Analysis and Transformation" href="Ch-3.html" /> 
  </head>

  <body>
    <div id="app">
    <div class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="../../../index.html" class="home-link">
    
      <span class="site-name">MLIR 中文文档</span>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">



    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            



            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">快速搜索</span>
    <div class="searchformwrapper">
      <form class="search" action="../../../search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="搜索" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="../../../index.html#mlir">欢迎使用 mlir 中文文档</a></span>
      </p>
      <ul class="current">
        
          <li class="toctree-l1 ">
            
              <a href="../../../_index.html" class="reference internal ">开始使用</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../CreatingADialect.html" class="reference internal ">Creating a Dialect</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../DataFlowAnalysis.html" class="reference internal ">Writing DataFlow Analyses in MLIR</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../QuickstartRewrites.html" class="reference internal ">Quickstart tutorial to adding MLIR graph rewrite</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="Ch-1.html" class="reference internal ">Chapter 1: Toy Language and AST</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="Ch-2.html" class="reference internal ">Chapter 2: Emitting Basic MLIR</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="Ch-3.html" class="reference internal ">Chapter 3: High-level Language-Specific Analysis and Transformation</a>
            

            
          </li>

        
          <li class="toctree-l1 current">
            
              <a href="#" class="reference internal current">Chapter 4: Enabling Generic Transformation with Interfaces</a>
            

            
              <ul>
                
                  <li class="toctree-l2"><a href="#background-grappling-with-an-extensible-ir" class="reference internal">Background: Grappling with an Extensible IR</a></li>
                
                  <li class="toctree-l2"><a href="#shape-inference-preparing-for-code-generation" class="reference internal">Shape Inference: Preparing for Code Generation</a></li>
                
              </ul>
            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="Ch-5.html" class="reference internal ">Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="Ch-6.html" class="reference internal ">Chapter 6: Lowering to LLVM and CodeGeneration</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="Ch-7.html" class="reference internal ">Chapter 7: Adding a Composite Type to Toy</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="_index.html" class="reference internal ">Toy Tutorial</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../UnderstandingTheIRStructure.html" class="reference internal ">Understanding the IR Structure</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../_index.html" class="reference internal ">Tutorials</a>
            

            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="../../../index.html">Docs</a> &raquo;</li>
    
    <li>Chapter 4: Enabling Generic Transformation with Interfaces</li>
  </ul>
  

  <ul class="page-nav">
  <li class="prev">
    <a href="Ch-3.html"
       title="上一章">← Chapter 3: High-level Language-Specific Analysis and Transformation</a>
  </li>
  <li class="next">
    <a href="Ch-5.html"
       title="下一章">Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization →</a>
  </li>
</ul>
  
</div>
<hr>
          <div class="content" role="main" v-pre>
            
  <section id="chapter-4-enabling-generic-transformation-with-interfaces">
<h1>Chapter 4: Enabling Generic Transformation with Interfaces<a class="headerlink" href="#chapter-4-enabling-generic-transformation-with-interfaces" title="此标题的永久链接">¶</a></h1>
<p>[TOC]</p>
<section id="background-grappling-with-an-extensible-ir">
<h2>Background: Grappling with an Extensible IR<a class="headerlink" href="#background-grappling-with-an-extensible-ir" title="此标题的永久链接">¶</a></h2>
<p>Through dialects, MLIR allows for the representation of many different levels of
abstraction; the Toy dialect that we have previously defined is one such
example. Though these different dialects may represent different abstractions,
there is often a set of common transformations and analyses that we would like
to perform. The problem that arises is that naively implementing each
transformation for each dialect leads to large amounts of code duplication, as
the internal algorithms are generally very similar, if not the same. We would
like to provide the ability for transformations to opaquely hook into dialects
like Toy to get the information they need.</p>
<p>MLIR provides a set of always available-hooks for certain core transformations,
as seen in the <a class="reference internal" href="Ch-3.html"><span class="doc">previous chapter</span></a>, where we registered some
canonicalizations via a hook on our operations (<code class="docutils literal notranslate"><span class="pre">getCanonicalizationPatterns</span></code>).
However, these types of hooks don’t really scale well. Therefore, a more generic
solution was designed, in the form of <a class="reference internal" href="../../Interfaces.html"><span class="doc">interfaces</span></a>, to make
the MLIR infrastructure as extensible as the representation. Interfaces provide
a generic mechanism for dialects and operations to provide information to a
transformation or analysis.</p>
</section>
<section id="shape-inference-preparing-for-code-generation">
<h2>Shape Inference: Preparing for Code Generation<a class="headerlink" href="#shape-inference-preparing-for-code-generation" title="此标题的永久链接">¶</a></h2>
<p>Our Toy IR currently operates on generic tensors, meaning that we don’t know the
shape of tensors other than during the initialization of constants. This
complicates optimizations, as well as code generation. Fortunately, we can
simply propagate the shapes through the computation until they are all known.
The issue is how to handle calls to user-defined generic functions: every call
site could deduce different shapes. One possibility would be to perform symbolic
inference based on the argument types, but this would be hard to generalize if
we were to introduce more control flow in the language. Another approach would
be function specialization, where every call site with new argument shapes
duplicates the called function and specializes it. The approach we take for Toy
is to inline all of the function calls, then perform intraprocedural shape
propagation.</p>
<section id="inlining">
<h3>Inlining<a class="headerlink" href="#inlining" title="此标题的永久链接">¶</a></h3>
<p>Here we could write an inlining algorithm specifically designed for the Toy
dialect, but that can become quite complicated depending on the level of
complexity that we want. Disregarding cost modeling, the pure structural
transformation is already complex to implement from scratch. Thankfully, MLIR
provides a generic inliner algorithm that dialects can plug into. All we need to
do in Toy is to provide the <a class="reference internal" href="../../Interfaces.html"><span class="doc">interfaces</span></a> for the inliner to
hook into.</p>
<p>The first thing we need to do is to define the constraints on inlining
operations in the Toy dialect. This information is provided through a
<a class="reference external" href="../../Interfaces.md/#dialect-interfaces">dialect interface</a>. This is essentially
a class containing a set of virtual hooks which the dialect can override.
In this case, the interface is <code class="docutils literal notranslate"><span class="pre">DialectInlinerInterface</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">/// This class defines the interface for handling inlining with Toy operations.</span>
<span class="c1">/// We simplify inherit from the base interface class and override</span>
<span class="c1">/// the necessary methods.</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">ToyInlinerInterface</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">DialectInlinerInterface</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">DialectInlinerInterface</span><span class="o">::</span><span class="n">DialectInlinerInterface</span><span class="p">;</span>

<span class="w">  </span><span class="c1">/// This hook checks to see if the given callable operation is legal to inline</span>
<span class="w">  </span><span class="c1">/// into the given call. For Toy this hook can simply return true, as the Toy</span>
<span class="w">  </span><span class="c1">/// Call operation is always inlinable.</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">isLegalToInline</span><span class="p">(</span><span class="n">Operation</span><span class="w"> </span><span class="o">*</span><span class="n">call</span><span class="p">,</span><span class="w"> </span><span class="n">Operation</span><span class="w"> </span><span class="o">*</span><span class="n">callable</span><span class="p">,</span>
<span class="w">                       </span><span class="kt">bool</span><span class="w"> </span><span class="n">wouldBeCloned</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">/// This hook checks to see if the given operation is legal to inline into the</span>
<span class="w">  </span><span class="c1">/// given region. For Toy this hook can simply return true, as all Toy</span>
<span class="w">  </span><span class="c1">/// operations are inlinable.</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">isLegalToInline</span><span class="p">(</span><span class="n">Operation</span><span class="w"> </span><span class="o">*</span><span class="p">,</span><span class="w"> </span><span class="n">Region</span><span class="w"> </span><span class="o">*</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="p">,</span>
<span class="w">                       </span><span class="n">IRMapping</span><span class="w"> </span><span class="o">&amp;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">/// This hook cheks if the given &#39;src&#39; region can be inlined into the &#39;dest&#39;</span>
<span class="w">  </span><span class="c1">/// region. The regions here are the bodies of the callable functions. For</span>
<span class="w">  </span><span class="c1">/// Toy, any function can be inlined, so we simply return true.</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">isLegalToInline</span><span class="p">(</span><span class="n">Region</span><span class="w"> </span><span class="o">*</span><span class="n">dest</span><span class="p">,</span><span class="w"> </span><span class="n">Region</span><span class="w"> </span><span class="o">*</span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">wouldBeCloned</span><span class="p">,</span>
<span class="w">                       </span><span class="n">IRMapping</span><span class="w"> </span><span class="o">&amp;</span><span class="n">valueMapping</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">/// This hook is called when a terminator operation has been inlined. The only</span>
<span class="w">  </span><span class="c1">/// terminator that we have in the Toy dialect is the return</span>
<span class="w">  </span><span class="c1">/// operation(toy.return). We handle the return by replacing the values</span>
<span class="w">  </span><span class="c1">/// previously returned by the call operation with the operands of the</span>
<span class="w">  </span><span class="c1">/// return.</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">handleTerminator</span><span class="p">(</span><span class="n">Operation</span><span class="w"> </span><span class="o">*</span><span class="n">op</span><span class="p">,</span>
<span class="w">                        </span><span class="n">ArrayRef</span><span class="o">&lt;</span><span class="n">Value</span><span class="o">&gt;</span><span class="w"> </span><span class="n">valuesToRepl</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Only &quot;toy.return&quot; needs to be handled here.</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">returnOp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cast</span><span class="o">&lt;</span><span class="n">ReturnOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Replace the values directly with the return operands.</span>
<span class="w">    </span><span class="n">assert</span><span class="p">(</span><span class="n">returnOp</span><span class="p">.</span><span class="n">getNumOperands</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">valuesToRepl</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">it</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">llvm</span><span class="o">::</span><span class="n">enumerate</span><span class="p">(</span><span class="n">returnOp</span><span class="p">.</span><span class="n">getOperands</span><span class="p">()))</span>
<span class="w">      </span><span class="n">valuesToRepl</span><span class="p">[</span><span class="n">it</span><span class="p">.</span><span class="n">index</span><span class="p">()].</span><span class="n">replaceAllUsesWith</span><span class="p">(</span><span class="n">it</span><span class="p">.</span><span class="n">value</span><span class="p">());</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
<p>Besides, the inliner will only discard private-visible unused function
definitions. We also have to set the visibility of functions (except the
main function) in the MLIR generator.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">/// Emit a new function and add it to the MLIR module.</span>
<span class="n">mlir</span><span class="o">::</span><span class="n">toy</span><span class="o">::</span><span class="n">FuncOp</span><span class="w"> </span><span class="nf">mlirGen</span><span class="p">(</span><span class="n">FunctionAST</span><span class="w"> </span><span class="o">&amp;</span><span class="n">funcAST</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="p">...</span>
<span class="w">  </span><span class="c1">// If this function isn&#39;t main, then set the visibility to private.</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">funcAST</span><span class="p">.</span><span class="n">getProto</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">getName</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="s">&quot;main&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">function</span><span class="p">.</span><span class="n">setPrivate</span><span class="p">();</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">function</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>We then register our dialect interface directly on the Toy dialect, similarly to
how we did for operations.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">ToyDialect::initialize</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">addInterfaces</span><span class="o">&lt;</span><span class="n">ToyInlinerInterface</span><span class="o">&gt;</span><span class="p">();</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Next, we need to provide a way for the inliner to know that <code class="docutils literal notranslate"><span class="pre">toy.generic_call</span></code>
represents a call, and <code class="docutils literal notranslate"><span class="pre">toy.func</span></code> represents a function. MLIR provides
<a class="reference external" href="../../Interfaces.md/#attributeoperationtype-interfaces">operation interfaces</a> that can be used
to mark an operation as being “call-like” or “callable-like”. Unlike dialect interfaces,
operation interfaces provide a more refined granularity of information that is specific
and core to a single operation. The interfaces that we will be adding here is the
<code class="docutils literal notranslate"><span class="pre">CallOpInterface</span></code> and <code class="docutils literal notranslate"><span class="pre">CallableOpInterface</span></code>.</p>
<p>To add this interface we just need to include the definition into our operation
specification file (<code class="docutils literal notranslate"><span class="pre">Ops.td</span></code>):</p>
<div class="highlight-tablegen notranslate"><div class="highlight"><pre><span></span>include &quot;mlir/Interfaces/CallInterfaces.td&quot;
</pre></div>
</div>
<p>and add it to the traits list of <code class="docutils literal notranslate"><span class="pre">GenericCallOp</span></code>:</p>
<div class="highlight-tablegen notranslate"><div class="highlight"><pre><span></span>def FuncOp : Toy_Op&lt;&quot;func&quot;,
    [DeclareOpInterfaceMethods&lt;CallableOpInterface&gt;]&gt; {
  ...
}

def GenericCallOp : Toy_Op&lt;&quot;generic_call&quot;,
    [DeclareOpInterfaceMethods&lt;CallOpInterface&gt;]&gt; {
  ...
}
</pre></div>
</div>
<p>In the above we also use the <code class="docutils literal notranslate"><span class="pre">DeclareOpInterfaceMethods</span></code> directive to
auto-declare all of the interface methods in the class declaration of
GenericCallOp. This means that we just need to provide a definition:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">/// Returns the region on the function operation that is callable.</span>
<span class="n">Region</span><span class="w"> </span><span class="o">*</span><span class="nf">FuncOp::getCallableRegion</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="o">&amp;</span><span class="n">getBody</span><span class="p">();</span><span class="w"> </span><span class="p">}</span>

<span class="c1">/// Returns the results types that the callable region produces when</span>
<span class="c1">/// executed.</span>
<span class="n">ArrayRef</span><span class="o">&lt;</span><span class="n">Type</span><span class="o">&gt;</span><span class="w"> </span><span class="n">FuncOp</span><span class="o">::</span><span class="n">getCallableResults</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">getType</span><span class="p">().</span><span class="n">getResults</span><span class="p">();</span><span class="w"> </span><span class="p">}</span>

<span class="c1">/// Returns the argument attributes for all callable region arguments or</span>
<span class="c1">/// null if there are none.</span>
<span class="n">ArrayAttr</span><span class="w"> </span><span class="n">FuncOp</span><span class="o">::</span><span class="n">getCallableArgAttrs</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">getArgAttrs</span><span class="p">().</span><span class="n">value_or</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">/// Returns the result attributes for all callable region results or</span>
<span class="c1">/// null if there are none.</span>
<span class="n">ArrayAttr</span><span class="w"> </span><span class="n">FuncOp</span><span class="o">::</span><span class="n">getCallableResAttrs</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">getResAttrs</span><span class="p">().</span><span class="n">value_or</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// ....</span>

<span class="c1">/// Return the callee of the generic call operation, this is required by the</span>
<span class="c1">/// call interface.</span>
<span class="n">CallInterfaceCallable</span><span class="w"> </span><span class="n">GenericCallOp</span><span class="o">::</span><span class="n">getCallableForCallee</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">getAttrOfType</span><span class="o">&lt;</span><span class="n">SymbolRefAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;callee&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">/// Get the argument operands to the called function, this is required by the</span>
<span class="c1">/// call interface.</span>
<span class="n">Operation</span><span class="o">::</span><span class="n">operand_range</span><span class="w"> </span><span class="n">GenericCallOp</span><span class="o">::</span><span class="n">getArgOperands</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">inputs</span><span class="p">();</span><span class="w"> </span><span class="p">}</span>
</pre></div>
</div>
<p>Now that the inliner has been informed about the Toy dialect, we can add the
inliner pass to the pass manager for Toy:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="n">pm</span><span class="p">.</span><span class="n">addPass</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">createInlinerPass</span><span class="p">());</span>
</pre></div>
</div>
<p>Now let’s look at a working example:</p>
<div class="highlight-mlir notranslate"><div class="highlight"><pre><span></span>toy.func @multiply_transpose(%arg0: tensor&lt;*xf64&gt;, %arg1: tensor&lt;*xf64&gt;) -&gt; tensor&lt;*xf64&gt; {
  %0 = toy.transpose(%arg0 : tensor&lt;*xf64&gt;) to tensor&lt;*xf64&gt;
  %1 = toy.transpose(%arg1 : tensor&lt;*xf64&gt;) to tensor&lt;*xf64&gt;
  %2 = toy.mul %0, %1 : tensor&lt;*xf64&gt;
  toy.return %2 : tensor&lt;*xf64&gt;
}
toy.func @main() {
  %0 = toy.constant dense&lt;[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]&gt; : tensor&lt;2x3xf64&gt;
  %1 = toy.reshape(%0 : tensor&lt;2x3xf64&gt;) to tensor&lt;2x3xf64&gt;
  %2 = toy.constant dense&lt;[1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]&gt; : tensor&lt;6xf64&gt;
  %3 = toy.reshape(%2 : tensor&lt;6xf64&gt;) to tensor&lt;2x3xf64&gt;
  %4 = toy.generic_call @multiply_transpose(%1, %3) : (tensor&lt;2x3xf64&gt;, tensor&lt;2x3xf64&gt;) -&gt; tensor&lt;*xf64&gt;
  %5 = toy.generic_call @multiply_transpose(%3, %1) : (tensor&lt;2x3xf64&gt;, tensor&lt;2x3xf64&gt;) -&gt; tensor&lt;*xf64&gt;
  toy.print %5 : tensor&lt;*xf64&gt;
  toy.return
}
</pre></div>
</div>
<p>We have two calls to multiply_transpose that we would like to inline into main,
but if we look at the output nothing has changed. We are missing one last subtle
piece: there is a hidden type conversion on the edge of the call. If we look at
the above, the operands to the generic_call are of type <code class="docutils literal notranslate"><span class="pre">tensor&lt;2x3xf64&gt;</span></code>, while
the inputs to the function expect <code class="docutils literal notranslate"><span class="pre">tensor&lt;*xf64&gt;</span></code>. To resolve this difference,
the inliner expects an explicit cast operation to be inserted. For this, we need
to add a new operation to the Toy dialect, <code class="docutils literal notranslate"><span class="pre">ToyCastOp</span></code>(toy.cast), to represent
casts between two different shapes.</p>
<div class="highlight-tablegen notranslate"><div class="highlight"><pre><span></span>def CastOp : Toy_Op&lt;&quot;cast&quot;, [
    DeclareOpInterfaceMethods&lt;CastOpInterface&gt;,
    Pure,
    SameOperandsAndResultShape]
  &gt; {
  let summary = &quot;shape cast operation&quot;;
  let description = [{
    The &quot;cast&quot; operation converts a tensor from one type to an equivalent type
    without changing any data elements. The source and destination types
    must both be tensor types with the same element type. If both are ranked,
    then shape is required to match. The operation is invalid if converting
    to a mismatching constant dimension.
  }];

  let arguments = (ins F64Tensor:$input);
  let results = (outs F64Tensor:$output);
  let assemblyFormat = &quot;$input attr-dict `:` type($input) `to` type($output)&quot;;
}
</pre></div>
</div>
<p>Note that the definition of this cast operation adds a <code class="docutils literal notranslate"><span class="pre">CastOpInterface</span></code> to the
traits list. This interface provides several utilities for cast-like operation,
such as folding identity casts and verification. We hook into this interface by
providing a definition for the <code class="docutils literal notranslate"><span class="pre">areCastCompatible</span></code> method:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">/// Returns true if the given set of input and result types are compatible with</span>
<span class="c1">/// this cast operation. This is required by the `CastOpInterface` to verify</span>
<span class="c1">/// this operation and provide other additional utilities.</span>
<span class="kt">bool</span><span class="w"> </span><span class="nf">CastOp::areCastCompatible</span><span class="p">(</span><span class="n">TypeRange</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">TypeRange</span><span class="w"> </span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">outputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// The inputs must be Tensors with the same element type.</span>
<span class="w">  </span><span class="n">TensorType</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">front</span><span class="p">().</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">TensorType</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="n">TensorType</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">outputs</span><span class="p">.</span><span class="n">front</span><span class="p">().</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">TensorType</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">input</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="o">!</span><span class="n">output</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">input</span><span class="p">.</span><span class="n">getElementType</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">output</span><span class="p">.</span><span class="n">getElementType</span><span class="p">())</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// The shape is required to match if both types are ranked.</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="o">!</span><span class="n">input</span><span class="p">.</span><span class="n">hasRank</span><span class="p">()</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="o">!</span><span class="n">output</span><span class="p">.</span><span class="n">hasRank</span><span class="p">()</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">output</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>With a proper cast operation, we can now override the necessary hook on the
ToyInlinerInterface to insert it for us when necessary:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">ToyInlinerInterface</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">DialectInlinerInterface</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="p">...</span>

<span class="w">  </span><span class="c1">/// Attempts to materialize a conversion for a type mismatch between a call</span>
<span class="w">  </span><span class="c1">/// from this dialect, and a callable region. This method should generate an</span>
<span class="w">  </span><span class="c1">/// operation that takes &#39;input&#39; as the only operand, and produces a single</span>
<span class="w">  </span><span class="c1">/// result of &#39;resultType&#39;. If a conversion can not be generated, nullptr</span>
<span class="w">  </span><span class="c1">/// should be returned.</span>
<span class="w">  </span><span class="n">Operation</span><span class="w"> </span><span class="o">*</span><span class="n">materializeCallConversion</span><span class="p">(</span><span class="n">OpBuilder</span><span class="w"> </span><span class="o">&amp;</span><span class="n">builder</span><span class="p">,</span><span class="w"> </span><span class="n">Value</span><span class="w"> </span><span class="n">input</span><span class="p">,</span>
<span class="w">                                       </span><span class="n">Type</span><span class="w"> </span><span class="n">resultType</span><span class="p">,</span>
<span class="w">                                       </span><span class="n">Location</span><span class="w"> </span><span class="n">conversionLoc</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">builder</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">CastOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">conversionLoc</span><span class="p">,</span><span class="w"> </span><span class="n">resultType</span><span class="p">,</span><span class="w"> </span><span class="n">input</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
<p>If we run the working example through the pipeline again, we get the expected:</p>
<div class="highlight-mlir notranslate"><div class="highlight"><pre><span></span>toy.func @main() {
  %0 = toy.constant dense&lt;[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]&gt; : tensor&lt;2x3xf64&gt;
  %1 = toy.constant dense&lt;[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]&gt; : tensor&lt;2x3xf64&gt;
  %2 = toy.cast %1 : tensor&lt;2x3xf64&gt; to tensor&lt;*xf64&gt;
  %3 = toy.cast %0 : tensor&lt;2x3xf64&gt; to tensor&lt;*xf64&gt;
  %4 = toy.transpose(%2 : tensor&lt;*xf64&gt;) to tensor&lt;*xf64&gt;
  %5 = toy.transpose(%3 : tensor&lt;*xf64&gt;) to tensor&lt;*xf64&gt;
  %6 = toy.mul %4, %5 : tensor&lt;*xf64&gt;
  toy.print %6 : tensor&lt;*xf64&gt;
  toy.return
}
</pre></div>
</div>
<p>NOTE: The generic inliner will also perform simplifications, so the output may
be a bit cleaner than expected.</p>
</section>
<section id="intraprocedural-shape-inference">
<h3>Intraprocedural Shape Inference<a class="headerlink" href="#intraprocedural-shape-inference" title="此标题的永久链接">¶</a></h3>
<p>Now that we have inlined all of the functions, we are left with a main function
containing a mix of static and dynamically shaped operations. We can now write a
simple shape inference pass to propagate shapes intraprocedurally (within a
single function). We could write this as a pass that directly encodes the
constraints of the operations within the Toy dialect, but this seems like a good
candidate for a transformation that could be written generically. As a good rule
of thumb, it is best to express a transformation as generically as possible,
such that it can be extended to other dialects in the future. There is no
telling how many other dialects may have similar needs or encounter the same
problems.</p>
<p>For shape inference, if we break down the problem to its core, we really just
want operations to tell us the expected outputs given a set of statically known
inputs. (We can definitely get more complex than that, but for our needs we can
keep it simple.) Given that this property is core to a specific operation, we
can define an operation interface that can be specified on operations that need
to have their result shapes inferred.</p>
<p>Similarly to operations, we can also
<a class="reference external" href="../../Interfaces.md/#attributeoperationtype-interfaces">define operation interfaces</a> using
the operation definition specification (ODS) framework.</p>
<p>The interface is defined by inheriting from <code class="docutils literal notranslate"><span class="pre">OpInterface</span></code>, which takes the name
to be given to the generated C++ interface class as a template argument. For our
purposes, we will simply name the generated class <code class="docutils literal notranslate"><span class="pre">ShapeInference</span></code>. We also
provide a description for the interface.</p>
<div class="highlight-tablegen notranslate"><div class="highlight"><pre><span></span>def ShapeInferenceOpInterface : OpInterface&lt;&quot;ShapeInference&quot;&gt; {
  let description = [{
    Interface to access a registered method to infer the return types for an
    operation that can be used during type inference.
  }];
}
</pre></div>
</div>
<p>Next, we define the interface methods that the operations will need to provide.
An interface method is comprised of: a description; a C++ return type in string
form; a method name in string form; and a few optional components, depending on
the need. See the
<a class="reference external" href="../../Interfaces.md/#attributeoperationtype-interfaces">ODS documentation</a> for more
information.</p>
<div class="highlight-tablegen notranslate"><div class="highlight"><pre><span></span>def ShapeInferenceOpInterface : OpInterface&lt;&quot;ShapeInference&quot;&gt; {
  ...

  let methods = [
    InterfaceMethod&lt;&quot;Infer and set the output shape for the current operation.&quot;,
                    &quot;void&quot;, &quot;inferShapes&quot;&gt;
  ];
}
</pre></div>
</div>
<p>Now that the interface is defined, we can add it to the necessary Toy operations
in a similar way to how we added the <code class="docutils literal notranslate"><span class="pre">CallOpInterface</span></code> to the GenericCallOp:</p>
<div class="highlight-tablegen notranslate"><div class="highlight"><pre><span></span>def MulOp : Toy_Op&lt;&quot;mul&quot;,
    [..., DeclareOpInterfaceMethods&lt;ShapeInferenceOpInterface&gt;]&gt; {
  ...
}
</pre></div>
</div>
<p>Each of these operations will then need to provide a definition for the
<code class="docutils literal notranslate"><span class="pre">inferShapes()</span></code> method. As an example, for the mul op, the result shape is
inferred as the shape of the inputs.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">/// Infer the output shape of the MulOp, this is required by the shape inference</span>
<span class="c1">/// interface.</span>
<span class="kt">void</span><span class="w"> </span><span class="nf">MulOp::inferShapes</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">getResult</span><span class="p">().</span><span class="n">setType</span><span class="p">(</span><span class="n">getLhs</span><span class="p">().</span><span class="n">getType</span><span class="p">());</span><span class="w"> </span><span class="p">}</span>
</pre></div>
</div>
<p>At this point, each of the necessary Toy operations provide a mechanism by which
to infer their output shapes. The ShapeInferencePass will operate on functions:
it will run on each function in isolation. MLIR also supports general
<a class="reference external" href="../../PassManagement.md#operation-pass">OperationPasses</a> that run on any
isolated operation, but here our module only contains functions, so there is no
need to generalize to all operations.</p>
<p>Implementing such a pass is done by creating a class inheriting from
<code class="docutils literal notranslate"><span class="pre">mlir::OperationPass&lt;FuncOp&gt;</span></code> and overriding the <code class="docutils literal notranslate"><span class="pre">runOnOperation()</span></code> method.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">ShapeInferencePass</span>
<span class="w">    </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">mlir</span><span class="o">::</span><span class="n">PassWrapper</span><span class="o">&lt;</span><span class="n">ShapeInferencePass</span><span class="p">,</span><span class="w"> </span><span class="n">OperationPass</span><span class="o">&lt;</span><span class="n">FuncOp</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">runOnOperation</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">FuncOp</span><span class="w"> </span><span class="n">function</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getOperation</span><span class="p">();</span>
<span class="w">    </span><span class="p">...</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
<p>While at it, let’s also create a helper method for instantiating the pass:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">Pass</span><span class="o">&gt;</span><span class="w"> </span><span class="n">mlir</span><span class="o">::</span><span class="n">toy</span><span class="o">::</span><span class="n">createShapeInferencePass</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">ShapeInferencePass</span><span class="o">&gt;</span><span class="p">();</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The shape inference algorithm operates as follows:</p>
<ol class="simple">
<li><p>Build a worklist containing all the operations that return a dynamically
shaped tensor: these are the operations that need shape inference.</p></li>
<li><p>Iterate on the worklist:</p>
<ul class="simple">
<li><p>find an operation to process: the next ready operation in the worklist
has all of its arguments non-generic,</p></li>
<li><p>if no operation is found, break out of the loop,</p></li>
<li><p>remove the operation from the worklist,</p></li>
<li><p>infer the shape of its output from the argument types.</p></li>
</ul>
</li>
<li><p>If the worklist is empty, the algorithm succeeded.</p></li>
</ol>
<p>When processing an operation like described, we query if it registered the
<code class="docutils literal notranslate"><span class="pre">ShapeInference</span></code> interface, using this code snippet:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Ask the operation to infer its output shapes.</span>
<span class="w">  </span><span class="n">LLVM_DEBUG</span><span class="p">(</span><span class="n">llvm</span><span class="o">::</span><span class="n">dbgs</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Inferring shape for: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="o">*</span><span class="n">op</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>

<span class="w">  </span><span class="c1">/// We check if an operation has a particular interface by casting.</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ShapeInference</span><span class="w"> </span><span class="n">shapeOp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">ShapeInference</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">shapeOp</span><span class="p">.</span><span class="n">inferShapes</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">op</span><span class="o">-&gt;</span><span class="n">emitError</span><span class="p">(</span><span class="s">&quot;unable to infer shape of operation without shape &quot;</span>
<span class="w">                  </span><span class="s">&quot;inference interface&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">signalPassFailure</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
<p>We can then add our pass to the pass manager:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="n">pm</span><span class="p">.</span><span class="n">addPass</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">createShapeInferencePass</span><span class="p">());</span>
</pre></div>
</div>
<p>If we rerun our original example, we now get the following:</p>
<div class="highlight-mlir notranslate"><div class="highlight"><pre><span></span>toy.func @main() {
  %0 = toy.constant dense&lt;[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]&gt; : tensor&lt;2x3xf64&gt;
  %1 = toy.transpose(%0 : tensor&lt;2x3xf64&gt;) to tensor&lt;3x2xf64&gt;
  %2 = toy.mul %1, %1 : tensor&lt;3x2xf64&gt;
  toy.print %2 : tensor&lt;3x2xf64&gt;
  toy.return
}
</pre></div>
</div>
<p>You can build <code class="docutils literal notranslate"><span class="pre">toyc-ch4</span></code> and try yourself: <code class="docutils literal notranslate"><span class="pre">toyc-ch4</span> <span class="pre">test/Examples/Toy/Ch4/codegen.toy</span> <span class="pre">-emit=mlir</span> <span class="pre">-opt</span></code>.</p>
<p>In the <a class="reference internal" href="Ch-5.html"><span class="doc">next chapter</span></a>, we will start the process of code generation by
targeting a lower level dialect for optimizing some of the more compute-heavy
Toy operations.</p>
</section>
</section>
</section>


          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
  <li class="prev">
    <a href="Ch-3.html"
       title="上一章">← Chapter 3: High-level Language-Specific Analysis and Transformation</a>
  </li>
  <li class="next">
    <a href="Ch-5.html"
       title="下一章">Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization →</a>
  </li>
</ul><div class="footer" role="contentinfo">
      &#169; 版权所有 2023, yaoyue123.
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 6.1.3 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a> 0.8.0.
</div>
            </div>
          </div>
      </page>
    </div></div>
    
    
  </body>
</html>